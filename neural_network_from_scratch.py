# -*- coding: utf-8 -*-
"""first-neural-network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FfbwYdi1y6b23gnhdWT9k-NB85g4wknX
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score,confusion_matrix

# Load the Pima Indian Diabetes dataset
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']
dataset = pd.read_csv(url, names=column_names)

# Define the number of folds for cross-validation
n_splits = 5

# Create a StratifiedKFold object
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Split the dataset into features (X) and target variable (y)
X = dataset.iloc[:, 0:8].values
y = dataset.iloc[:, 8].values

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the learning rate and optimizer
#learning_rate = 0.001
#optimizer = Adam(learning_rate=learning_rate)

# Build the neural network model
def build_model():
    model = Sequential()
    model.add(Dense(8, input_dim=8, activation='relu'))
    model.add(Dense(12, activation='relu'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


# Initialize an array to store cross-validation scores
cv_scores = []
conf_matrices = []

# Perform cross-validation
for train_index, test_index in kf.split(X, y):
    X_train, X_val = X[train_index], X[test_index]
    y_train, y_val = y[train_index], y[test_index]

    model = build_model()

    # Train the model
    model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)

    # Make predictions on the validation set
    y_pred = (model.predict(X_val) > 0.5).astype(int)

    # Calculate accuracy and store the score
    accuracy = accuracy_score(y_val, y_pred)
    cv_scores.append(accuracy)
    # Calculate confusion matrix and store it
    conf_matrix = confusion_matrix(y_val, y_pred)
    conf_matrices.append(conf_matrix)

# Print the mean and standard deviation of the cross-validation scores
print(f'Mean Cross-Validation Accuracy: {np.mean(cv_scores):.4f}')
print(f'Standard Deviation of Cross-Validation Accuracy: {np.std(cv_scores):.4f}')

# Print confusion matrices for each fold
for i, conf_matrix in enumerate(conf_matrices):
    print(f'\nConfusion Matrix (Fold {i + 1}):\n{conf_matrix}')

from tensorflow.keras.utils import plot_model
plot_model(model, to_file='neural_network_architecture.png', show_shapes=True)

fig, ax = plt.subplots(figsize=(7.5, 7.5))
ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

# New input data for prediction
new_data = np.array([[1, 150, 72, 35, 0, 33.6, 0.627, 50]])

# Standardize the new input data using the same scaler used for training
new_data_scaled = scaler.transform(new_data)

# Make predictions
predictions = model.predict(new_data_scaled)

# Round the probabilities to get binary predictions
binary_predictions = (predictions > 0.5).astype(int)

# Print the binary predictions
print(f'Prediction Result: {binary_predictions[0, 0]}')

import matplotlib.pyplot as plt

# Assuming 'model' is the Sequential model
# Assuming X_train and y_train are your training data
# Assuming X_test and y_test are your test data

# Train the model and get the history
history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test), verbose=0)

# Plot training loss vs. validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training accuracy vs. validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()